# Lyrebird â€” Log-to-Song Pipeline for Explainable GenAI Workflows

Lyrebird is a production-oriented Generative AI application built for the AWS x Datadog hackathon. It transforms team chat logs into a factual, auditable artifact: extract â†’ review â†’ sanitize â†’ graph â†’ music.

## One-line summary

Lyrebird converts raw conversation transcripts into verified factual statements, applies human-guided find/replace validation, persists lineage in a knowledge graph, and outputs a playable lyric-driven music artifact.

## Hackathon fit

Lyrebird satisfies the eventâ€™s core constraints:

- AWS infrastructure stack with Bedrock, Strands Agents, and Amazon Bedrock AgentCore Runtime
- Datadog observability integration with traces, dashboards, and runtime quality signals
- Live, production-shaped end-to-end demo with replayable pipeline stages
- Sponsor track implementation for MiniMax, Neo4j, TestSprite, and CopilotKit

## User flow

### High-level flow

```mermaid
flowchart TD
  C[ğŸ“„ Context]
  P[ğŸ‘¤ Profile]

  subgraph Egg["ğŸ¥š Egg"]
    Y[ğŸ§¬ Facts extraction]
    A[ğŸ³ Semantic batch find and replace]
    R{ğŸ” again}
    F["âœ… Final facts"]
  end

  C --> Y
  P --> Y
  Y --> A
  A --> R
  R -->|âœ… Yes| A
  R -->|âŒ No| F
  F --> G[ğŸ•¸ï¸ Graph]
  G --> M[ğŸ§ MiniMax song generation]
```

### What this means for judges

1. Users start in **Egg** to produce a run context from either generated or pasted conversation.
2. **Context** and **Profile** feed into **Yolk**, which extracts candidate facts with provenance and confidence.
3. **Albumen** applies configurable pass rules to sanitized facts.
4. **Need another pass?** controls iteration until the team accepts **Final facts**.
5. **Graph** renders final fact lineage and then flows into **Music** generation from the approved artifact.

## System architecture

```mermaid
flowchart LR
  subgraph Client["Frontend"]
    FE["ğŸ§© React + Vite UI (CopilotKit styled workflow)"] 
  end

  subgraph Orchestration["Workflow control"]
    SFn["âš™ï¸ AWS Step Functions<br/>Eggâ†’Yolkâ†’Albumenâ†’Graphâ†’Music"]
    API["ğŸŒ Run API Layer"]
  end

  subgraph Agents["Reasoning and transformation"]
    AgentCore["ğŸ§  Amazon Bedrock AgentCore<br/>(Strands Agents runtime)"]
    Bedrock["ğŸ§® Amazon Bedrock models"]
    MiniMax["ğŸ¼ MiniMax Music Generation<br/>/v1/music_generation"]
  end

  subgraph Data["Storage and graph"]
    DDB["ğŸ—‚ï¸ DynamoDB run state & versions"]
    S3["ğŸ—„ï¸ S3 artifacts (lyrics/audio metadata)"]
    Neo4j["ğŸ•¸ï¸ Neo4j Aura graph"]
  end

  subgraph Ops["Observability"]
    DDTrace["ğŸ“Š Datadog LLM Observability"]
    DDDash["ğŸ“ˆ Datadog Dashboards"]
    DDmcp["ğŸ› ï¸ Datadog MCP debug bridge"]
  end

  User((ğŸ‘©â€âš–ï¸ Judge / ğŸ‘¤ User)) --> FE
  FE --> API
  API --> SFn
  SFn --> AgentCore --> Bedrock
  SFn --> Neo4j
  SFn --> MiniMax
  SFn --> DDB
  SFn --> S3
  AgentCore --> DDTrace
  MiniMax --> DDTrace
  DDB --> FE
  S3 --> FE
  Neo4j --> FE
  DDTrace --> DDDash
  DDmcp --> DDTrace
  DDDash --> FE
  DDmcp --> FE
```

### Architecture notes

- **Frontend**: single-stage Copilot-like interface with stage timeline, fact review, albumen passes, graph visualization, and song playback.
- **Orchestrator**: AWS Step Functions provides deterministic stage transitions, retries, and deterministic run versioning.
- **Agent layer**: Strands Agents on Bedrock AgentCore handles fact extraction and structured fact workflows.
- **Persistent state**: run metadata and lifecycle status in DynamoDB, generated artifacts in S3, and graph lineage in Neo4j.
- **Music service**: MiniMax API generates instrumentals from selected lyric context.
- **Observability**: Datadog captures trace spans for every stage plus latency/error/quality signals and exposes a user-facing debug surface.
- **Quality control**: stage-level telemetry is continuously available and exportable for judging.

## Sponsor usage in final solution

### MiniMax

MiniMax powers the **Song** stage:

- Converts approved factual content into a music artifact through `POST /v1/music_generation`
- Uses configurable prompt and tone inputs
- Returns playable audio as run output with associated metadata

- Required request shape (from `api.minimax.io`):
  - `POST /v1/music_generation`
  - Headers: `Authorization: Bearer <MINIMAX_API_KEY>`, `Content-Type: application/json`
  - Body:
    - `model` (required): `"music-2.5"`
    - `prompt` (optional for music-2.5): style / mood description
    - `lyrics` (required): structured lyric text with tags like `[Intro]`, `[Verse]`, `[Chorus]`, `[Outro]`
    - `output_format` (optional): `"url"` or `"hex"` (default `"hex"`)
    - `audio_setting`: `{ sample_rate, bitrate, format: "mp3" }`

- This repo uses two-step generation in the mock backend:
  1. optional `POST /v1/lyrics_generation` (`mode: "write_full_song"`) to draft lyrics when a user asks for MiniMax.
  2. `POST /v1/music_generation` to produce the final playback artifact.

- Configure locally with:
  - `MINIMAX_API_KEY` or `MINMAX_API_KEY`
  - `MINIMAX_API_HOST` (defaults to `https://api.minimax.io`)
  - `MINIMAX_MOCK_ONLY=true` to force local deterministic tone fallback

### Neo4j

Neo4j stores run provenance as a graph:

- Message â†’ Fact â†’ Albumen pass â†’ Song relationships
- Enables auditability of why each fact exists and how each transform changed it
- Powers graph explorer and debugging context in UI

### TestSprite

TestSprite is used for productized QA:

- E2E coverage of Egg â†’ Yolk â†’ Albumen â†’ Graph â†’ Song
- API-level regression testing for run lifecycle endpoints
- Failure-path coverage and test result reporting for confidence before judging

### CopilotKit

CopilotKit patterns shape the entire user-facing experience:

- Stage-based interactive controls and streaming action-oriented interface
- Fact review and transform controls as structured UI components
- Clear human-in-the-loop feedback loops over extracted and transformed content

## Run locally

```bash
npm install
cp .env.example .env
npm run dev
```

Then open:

```text
http://localhost:5173
```

## API endpoints

- `POST /api/run/egg`
- `POST /api/run/:runId/yolk`
- `POST /api/run/:runId/albumen`
- `POST /api/run/:runId/graph`
- `POST /api/run/:runId/music`
- `GET /api/run/:runId/debug`
- `GET /api/run/:runId/export`

## Scripts

- `npm run dev` â€” run local API + frontend
- `npm run dev:server` â€” run API service
- `npm run dev:client` â€” run frontend only
- `npm run build` â€” production build
- `npm run preview` â€” serve build output

## Project artifacts

- `src/App.tsx` â€“ client workflow implementation and stage-driven screens
- `src/server/` â€“ run orchestration API implementation
- `src/services/api.ts` â€” typed run APIs
- `src/types.ts` â€” typed run state, facts, passes, graph, and telemetry schemas
- `prd.md` â€” full problem framing, requirements, acceptance criteria

## Why judges should review Lyrebird

Lyrebird delivers an end-to-end, traceable, explainable pipeline that demonstrates both **LLM engineering maturity** and **system production qualities**: auditable extraction, explicit human control, graph lineage, structured retries, and telemetry-driven debugging.

## TestSprite Dashboard Setup (Why â€œNo Test Createdâ€ appears)

If the dashboard shows `No Test Created`, there are no generated test cases for this repo yet.

Use this exact flow after `npm run testsprite:mcp` is running:

1. `testsprite_bootstrap`
   - `projectPath`: `/home/kirill/hachathons/lyrebird-awsloft-hack260220`
   - `type`: `"frontend"`
   - `testScope`: `"codebase"`
   - `localPort`: `5173`

2. `testsprite_generate_code_summary`
3. `testsprite_generate_standardized_prd`
4. `testsprite_generate_frontend_test_plan`
5. `testsprite_generate_code_and_execute`

Then open:

```text
https://www.testsprite.com/dashboard
```

After step 5 completes, run:

- `testsprite_open_test_result_dashboard`

and refresh the dashboard list. Tests should now appear in `Recent Created Tests`.
